{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74975ac3",
   "metadata": {},
   "source": [
    "#### NLP | MVP\n",
    "\n",
    "# Coronavirus Tweets: April 2020<a id='top'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea017a0f",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "## **Analysis Goal**  \n",
    "The client, [name](link), ...\n",
    "wants to ... and \n",
    "requested a model to find a pattern ... in tweets in April. \n",
    "\n",
    "Impetus?\n",
    "\n",
    "**RQ:** ....?\n",
    "\n",
    "## **Process**\n",
    "**Data source:** \n",
    "Coronavirus COVID-19 Tweets [early](https://www.kaggle.com/datasets/smid80/coronavirus-covid19-tweets-early-april) and [late](https://www.kaggle.com/datasets/smid80/coronavirus-covid19-tweets-late-april) April\n",
    "\n",
    "\n",
    "**Models:** \n",
    "* [name] model, fit a [model name](#??):\n",
    "    * eval metric\n",
    "* [name] model, fit a [model name](#??):\n",
    "    * eval metric\n",
    "\n",
    "\n",
    "## **Preliminary Visualizations**\n",
    "[Figure 1. Name](#??)<br/>\n",
    "[Figure 1. Name](#??)<br/>\n",
    "\n",
    "## **Preliminary Conclusions**\n",
    "The [name] model ... showed....<br/>\n",
    "\n",
    "The next step .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64179b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['retina']  # or svg\n",
    "sns.set(context='notebook', style='whitegrid')\n",
    "\n",
    "from itertools import cycle\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "\n",
    "\n",
    "from cleantext import clean\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38894c8",
   "metadata": {},
   "source": [
    "## 1 | Dataset: Coronavirus Tweets ([early](https://www.kaggle.com/datasets/smid80/coronavirus-covid19-tweets-early-april) and [late](https://www.kaggle.com/datasets/smid80/coronavirus-covid19-tweets-late-april) April) <a id='1'></a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import one csv file to see data\n",
    "# single_df = pd.read_csv('20200401.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the following 7 columns on full data import \n",
    "\n",
    "#  2   created_at            591480 non-null  object \n",
    "#  3   screen_name           591480 non-null  object \n",
    "#  4   text                  591480 non-null  object \n",
    "#  13  country_code          26040 non-null   object \n",
    "#  18  account_lang          0 non-null       float64\n",
    "#  20  verified              591480 non-null  bool   \n",
    "#  21  lang                  591480 non-null  object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read list of April 2020 files, while ommitting columns above\n",
    "\n",
    "# path = r'/Users/sandraparedes/Documents/GitHub/metis_dsml/05_nlp/g00-nlp-project/data' \n",
    "# all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# li = []\n",
    "\n",
    "# for filename in all_files:\n",
    "#     df = pd.read_csv(filename, index_col=None, header=0, usecols=[2,3,4,13,18,20,21])\n",
    "#     li.append(df)\n",
    "# df = pd.concat(li, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678fce99",
   "metadata": {},
   "source": [
    "## 2 | Exploratory Data Analysis<a id='2'></a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['country_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['lang'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454cdc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('English entries:', (df[df[\"lang\"] == 'en'].count())['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('US entries:', (df[df[\"country_code\"] == 'US'].count())['country_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_us_df = df[(df['country_code'] == 'US') &\n",
    "#                (df['lang'] == 'en')]\n",
    "# eng_us_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_us_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908cc28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_us_df['text'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d83d4",
   "metadata": {},
   "source": [
    "### English + US df: 138,789 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b578af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save cleaned df as tweet_df\n",
    "\n",
    "# tweet_df = eng_us_df \n",
    "# tweet_df.to_pickle('coronavirus_tweets_df.pkl')\n",
    "# tweet_df.to_csv(r'//Users/sandraparedes/Documents/GitHub/metis_dsml/05_nlp/g00-nlp-project/data/coronavirus_tweets_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c61163bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>account_lang</th>\n",
       "      <th>verified</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-06T00:00:05Z</td>\n",
       "      <td>WFMGINC</td>\n",
       "      <td>....#SUNDAYFUNDAY #coronavirus style #vino cheers üç∑ https://t.co/SrymChBkq2</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-06T00:00:14Z</td>\n",
       "      <td>jpomietlasz</td>\n",
       "      <td>This pandemic has confirmed my worst fears, most people don‚Äôt know how to make entertaining videos. #Covid_19 #SinceIveBeenQuarantined #AmericasUnfunniestVideos #WrestleMania #tonyaharding</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-06T00:00:17Z</td>\n",
       "      <td>harold0719</td>\n",
       "      <td>Is this true? \\nhttps://t.co/Xj2zdFe7ab\\n#EcuadorEnEmergencia #Covid_19 #CoronavirusPandemic</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-06T00:00:18Z</td>\n",
       "      <td>uche_blackstock</td>\n",
       "      <td>Many us thought it was Wuhan province, but it could never be us. Then it was Italy, but it could never be us. Now it is here. One #NewYorker died every 12 minutes from #COVID19 over this weekend. Absolutely devastating.\\n\\nhttps://t.co/h5woMTfSCH</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-06T00:00:18Z</td>\n",
       "      <td>BrentALang</td>\n",
       "      <td>Ah #coronavirus humor https://t.co/1yXAZXNIgz</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-04-06T00:00:18Z</td>\n",
       "      <td>grantstern</td>\n",
       "      <td>Miami and South Florida in general are also staying home.\\n\\nNorth Florida thinks their immune to #COVID19. https://t.co/OiPfVqyMvC</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-04-06T00:00:20Z</td>\n",
       "      <td>Thom_Cordeiro</td>\n",
       "      <td>How can President Trump be flip about a question about continuity of power and contracting the #coronavirus? ‚ÄúWhen I am around him I don‚Äôt breathe‚Äù? And we wonder why other Americans question and defy the quarantine. #CoronavirusTaskForce</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-04-06T00:00:24Z</td>\n",
       "      <td>shelbalynn17</td>\n",
       "      <td>This is our most desperate hour. Help us, Obi-Wan Kenobi. You're our only hope. #Covid_19 #StarWars #weneedaleader</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-04-06T00:00:25Z</td>\n",
       "      <td>LheimIndustries</td>\n",
       "      <td>Face your fears....\\n#business #love #coronavirus #covid19 #nyc #ny #la #California #zoom #happy #prayer https://t.co/idDV3bpkUr</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-04-06T00:00:28Z</td>\n",
       "      <td>Elliot_Fuchs</td>\n",
       "      <td>First Defense Nasal Screen coming up on @ABCSharkTank during the #Covid_19 outbreak and all of a sudden the guy doesn‚Äôt seem crazy.\\n\\nSharks redeemed themselves w/ mega-offers.\\n\\nPost Corona-airing, @mcuban @TheSharkDaymond @kevinolearytv is gonna make a killing. Replaces mask-need</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             created_at      screen_name  \\\n",
       "0  2020-04-06T00:00:05Z          WFMGINC   \n",
       "1  2020-04-06T00:00:14Z      jpomietlasz   \n",
       "2  2020-04-06T00:00:17Z       harold0719   \n",
       "3  2020-04-06T00:00:18Z  uche_blackstock   \n",
       "4  2020-04-06T00:00:18Z       BrentALang   \n",
       "5  2020-04-06T00:00:18Z       grantstern   \n",
       "6  2020-04-06T00:00:20Z    Thom_Cordeiro   \n",
       "7  2020-04-06T00:00:24Z     shelbalynn17   \n",
       "8  2020-04-06T00:00:25Z  LheimIndustries   \n",
       "9  2020-04-06T00:00:28Z     Elliot_Fuchs   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                           text  \\\n",
       "0                                                                                                                                                                                                                   ....#SUNDAYFUNDAY #coronavirus style #vino cheers üç∑ https://t.co/SrymChBkq2   \n",
       "1                                                                                                  This pandemic has confirmed my worst fears, most people don‚Äôt know how to make entertaining videos. #Covid_19 #SinceIveBeenQuarantined #AmericasUnfunniestVideos #WrestleMania #tonyaharding   \n",
       "2                                                                                                                                                                                                  Is this true? \\nhttps://t.co/Xj2zdFe7ab\\n#EcuadorEnEmergencia #Covid_19 #CoronavirusPandemic   \n",
       "3                                        Many us thought it was Wuhan province, but it could never be us. Then it was Italy, but it could never be us. Now it is here. One #NewYorker died every 12 minutes from #COVID19 over this weekend. Absolutely devastating.\\n\\nhttps://t.co/h5woMTfSCH   \n",
       "4                                                                                                                                                                                                                                                 Ah #coronavirus humor https://t.co/1yXAZXNIgz   \n",
       "5                                                                                                                                                           Miami and South Florida in general are also staying home.\\n\\nNorth Florida thinks their immune to #COVID19. https://t.co/OiPfVqyMvC   \n",
       "6                                                How can President Trump be flip about a question about continuity of power and contracting the #coronavirus? ‚ÄúWhen I am around him I don‚Äôt breathe‚Äù? And we wonder why other Americans question and defy the quarantine. #CoronavirusTaskForce   \n",
       "7                                                                                                                                                                            This is our most desperate hour. Help us, Obi-Wan Kenobi. You're our only hope. #Covid_19 #StarWars #weneedaleader   \n",
       "8                                                                                                                                                              Face your fears....\\n#business #love #coronavirus #covid19 #nyc #ny #la #California #zoom #happy #prayer https://t.co/idDV3bpkUr   \n",
       "9  First Defense Nasal Screen coming up on @ABCSharkTank during the #Covid_19 outbreak and all of a sudden the guy doesn‚Äôt seem crazy.\\n\\nSharks redeemed themselves w/ mega-offers.\\n\\nPost Corona-airing, @mcuban @TheSharkDaymond @kevinolearytv is gonna make a killing. Replaces mask-need   \n",
       "\n",
       "  country_code account_lang verified lang  \n",
       "0           US          NaN    False   en  \n",
       "1           US          NaN    False   en  \n",
       "2           US          NaN    False   en  \n",
       "3           US          NaN     True   en  \n",
       "4           US          NaN     True   en  \n",
       "5           US          NaN     True   en  \n",
       "6           US          NaN    False   en  \n",
       "7           US          NaN    False   en  \n",
       "8           US          NaN    False   en  \n",
       "9           US          NaN    False   en  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #read df in\n",
    "df = pd.read_csv('coronavirus_tweets_df.csv', low_memory=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef27231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>....#SUNDAYFUNDAY #coronavirus style #vino cheers üç∑ https://t.co/SrymChBkq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This pandemic has confirmed my worst fears, most people don‚Äôt know how to make entertaining videos. #Covid_19 #SinceIveBeenQuarantined #AmericasUnfunniestVideos #WrestleMania #tonyaharding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           text\n",
       "0                                                                                                                   ....#SUNDAYFUNDAY #coronavirus style #vino cheers üç∑ https://t.co/SrymChBkq2\n",
       "1  This pandemic has confirmed my worst fears, most people don‚Äôt know how to make entertaining videos. #Covid_19 #SinceIveBeenQuarantined #AmericasUnfunniestVideos #WrestleMania #tonyaharding"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['created_at', 'screen_name', 'country_code', \n",
    "                 'account_lang', 'verified', 'lang'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233740c",
   "metadata": {},
   "source": [
    "### 2a | [start here with nlp](http://localhost:8888/notebooks/Documents/GitHub/metis_dsml/05_nlp/g1-nlp-overview/NLP_Supervised_Learning.ipynb)<a id='2a'></a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bdb5a4",
   "metadata": {},
   "source": [
    "### Remove numbers, capital letters, and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6219ab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sundayfunday  coronavirus style  vino cheers üç∑ https   t co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this pandemic has confirmed my worst fears  most people don‚Äôt know how to make entertaining videos      sinceivebeenquarantined  americasunfunniestvideos  wrestlemania  tonyaharding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                    text\n",
       "0                                                                                                                          sundayfunday  coronavirus style  vino cheers üç∑ https   t co  \n",
       "1  this pandemic has confirmed my worst fears  most people don‚Äôt know how to make entertaining videos      sinceivebeenquarantined  americasunfunniestvideos  wrestlemania  tonyaharding"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*',' ', str(x))\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "                          \n",
    "df['text'] = df.text.map(alphanumeric).map(punc_lower)\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35067b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emojis\n",
    "df = df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8038e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sundayfunday  coronavirus style  vino cheers  https   t co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this pandemic has confirmed my worst fears  most people dont know how to make entertaining videos      sinceivebeenquarantined  americasunfunniestvideos  wrestlemania  tonyaharding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                   text\n",
       "0                                                                                                                          sundayfunday  coronavirus style  vino cheers  https   t co  \n",
       "1  this pandemic has confirmed my worst fears  most people dont know how to make entertaining videos      sinceivebeenquarantined  americasunfunniestvideos  wrestlemania  tonyaharding"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f537a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save clean df\n",
    "\n",
    "clean_covid_tweets_df = df \n",
    "clean_covid_tweets_df.to_pickle('clean_covid_tweets_df.pkl')\n",
    "clean_covid_tweets_df.to_csv(r'//Users/sandraparedes/Documents/GitHub/metis_dsml/05_nlp/g00-nlp-project/clean_covid_tweets_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read df in\n",
    "# df = pd.read_csv('clean_covid_tweets_df.csv', low_memory=False)\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a4fd41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>america</th>\n",
       "      <th>americans</th>\n",
       "      <th>amp</th>\n",
       "      <th>april</th>\n",
       "      <th>away</th>\n",
       "      <th>best</th>\n",
       "      <th>better</th>\n",
       "      <th>business</th>\n",
       "      <th>california</th>\n",
       "      <th>care</th>\n",
       "      <th>...</th>\n",
       "      <th>watch</th>\n",
       "      <th>way</th>\n",
       "      <th>week</th>\n",
       "      <th>weeks</th>\n",
       "      <th>work</th>\n",
       "      <th>workers</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>york</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138791</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138792</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138793</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138794</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138795</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138796 rows √ó 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        america  americans  amp  april  away  best  better  business  \\\n",
       "0             0          0    0      0     0     0       0         0   \n",
       "1             0          0    0      0     0     0       0         0   \n",
       "2             0          0    0      0     0     0       0         0   \n",
       "3             0          0    0      0     0     0       0         0   \n",
       "4             0          0    0      0     0     0       0         0   \n",
       "...         ...        ...  ...    ...   ...   ...     ...       ...   \n",
       "138791        0          0    0      0     0     0       0         0   \n",
       "138792        0          0    0      0     0     0       0         0   \n",
       "138793        0          0    0      0     0     0       0         0   \n",
       "138794        0          0    0      0     0     0       0         0   \n",
       "138795        0          0    0      0     0     0       0         0   \n",
       "\n",
       "        california  care  ...  watch  way  week  weeks  work  workers  \\\n",
       "0                0     0  ...      0    0     0      0     0        0   \n",
       "1                0     0  ...      0    0     0      0     0        0   \n",
       "2                0     0  ...      0    0     0      0     0        0   \n",
       "3                0     0  ...      0    0     0      0     0        0   \n",
       "4                0     0  ...      0    0     0      0     0        0   \n",
       "...            ...   ...  ...    ...  ...   ...    ...   ...      ...   \n",
       "138791           0     0  ...      0    0     0      0     0        0   \n",
       "138792           1     0  ...      0    0     0      0     0        0   \n",
       "138793           0     0  ...      0    0     0      0     0        0   \n",
       "138794           0     0  ...      0    0     0      0     0        0   \n",
       "138795           0     0  ...      0    0     0      0     0        0   \n",
       "\n",
       "        working  world  year  york  \n",
       "0             0      0     0     0  \n",
       "1             0      0     0     0  \n",
       "2             0      0     0     0  \n",
       "3             0      0     0     0  \n",
       "4             0      0     0     0  \n",
       "...         ...    ...   ...   ...  \n",
       "138791        0      0     0     0  \n",
       "138792        0      0     0     0  \n",
       "138793        0      0     0     0  \n",
       "138794        0      0     0     0  \n",
       "138795        0      0     0     0  \n",
       "\n",
       "[138796 rows x 146 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer with min/max\n",
    "\n",
    "X = df.text\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', min_df=0.01, max_df=.95)\n",
    "\n",
    "text_cv = cv.fit_transform(X)\n",
    "\n",
    "text_df_cv = pd.DataFrame(text_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "text_df_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb851b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_name_is_sandra = cv.transform(['week workers'])\n",
    "# my_name_is_sandra.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b677cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_cv.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a60f4",
   "metadata": {},
   "source": [
    "### 3 | [sentiment analysis](http://localhost:8888/notebooks/Documents/GitHub/metis_dsml/05_nlp/g1-nlp-overview/nlp_unsupervised_nlp_exercises7.ipynb) code blocks<a id='3'></a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer() \n",
    "sentiment = analyzer.polarity_scores(df).get('compound')\n",
    "print('compound', sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = df.text.map(analyzer.polarity_scores).map(lambda x: x.get('compound'))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ed1a1",
   "metadata": {},
   "source": [
    "### 4 | [dim redux](http://localhost:8888/notebooks/Documents/GitHub/metis_dsml/05_nlp/g2-dimensionality-reduction/nlp_unsupervised_dimensionality_reduction_exercises3.ipynb) and [PCA](http://localhost:8888/notebooks/Documents/GitHub/metis_dsml/05_nlp/g2-dimensionality-reduction/PCA_digits.ipynb)<a id='4'></a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22abdcb",
   "metadata": {},
   "source": [
    "# bookmark Thurs evening\n",
    "# kernel quits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79c24341",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_5 = PCA(n_components=5)\n",
    "pca_5.fit(text_cv.toarray())\n",
    "pcafeatures = pca_5.transform(text_cv.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9117c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcafeatures = pca_5.transform(text_cv.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a55831c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138796, 146)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cv.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "169c11c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 146)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_5_components = pca_5.components_\n",
    "pca_5_components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc5_positives = []\n",
    "pc5_positives.append(text_df_cv1.columns[4])\n",
    "pc5_positives.append(text_df_cv1.columns[5])\n",
    "pc5_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45920735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ceea84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a57bae2e",
   "metadata": {},
   "source": [
    "### 4 | [LSA + NMF](http://localhost:8888/notebooks/Documents/GitHub/metis_dsml/05_nlp/g3-topic-modeling/Topic_Modeling_LSA_NMF.ipynb) [LSA chapter ex](http://localhost:8888/notebooks/Documents/GitHub/metis_dsml/05_nlp/g3-topic-modeling/nlp_unsupervised_topic_modeling_exercises2.ipynb), [NMF exercise](http://localhost:8888/notebooks/Documents/GitHub/metis_dsml/05_nlp/g01-nlp-exercises/05-nmf/nmf_solution-1.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fdb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_term       V visible variables      input (corpus matrix)\n",
    "# doc_topic      W weights                feature set\n",
    "# topic_term     H hidden weights         coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c00dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa359cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "# define model, pass n_components\n",
    "# define doc_topic_W, pass doc_term_V\n",
    "\n",
    "nmf_5 = NMF(n_components=5)\n",
    "doc_topic = nmf_5.fit_transform(doc_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca18e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_5.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass topic, n_terms, model, terms\n",
    "\n",
    "def get_top_terms(topic, n_terms, nmf=nmf_5, terms=terms):\n",
    "    # get the topic components (i.e., term weights)\n",
    "    components = nmf.components_[topic, :]\n",
    "\n",
    "    # get term indices, sorted (descending) by topic weights\n",
    "    top_term_indices = components.argsort()[-n_terms:]\n",
    "    \n",
    "    # use the `terms` array to get the actual top terms\n",
    "    top_terms = np.array(terms)[top_term_indices]\n",
    "    \n",
    "    return top_terms.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_terms(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed97556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the topics\n",
    "# pass function above\n",
    "\n",
    "topics = ['-'.join(get_top_terms(i, 3)) for i in range(5)]\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the categories\n",
    "categories = [d[:d.find('.')] for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538e3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74926c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['politics', 'entertainment', 'sport', 'tech', 'business']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be973ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd33b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "# define vectorizer\n",
    "# define doc_term_V, pass corpus\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "doc_term = vectorizer.fit_transform(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a4f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass doc_term_V, index, vectorizer\n",
    "\n",
    "pd.DataFrame(doc_term.toarray(), \n",
    "             index=ex_label, \n",
    "             columns=vectorizer.get_feature_names()).head(10)\n",
    "\n",
    "# ex_label = [e[:30]+\"...\" for e in example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e91acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model with n topics\n",
    "# define doc_topic_W, pass model + doc_term_V\n",
    "\n",
    "nmf_2 = NMF(2)\n",
    "doc_topic = nmf_model.fit_transform(doc_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define topic_word, index, cols, pass model + components_ \n",
    "\n",
    "topic_word = pd.DataFrame(nmf_2.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\"],\n",
    "             columns = vectorizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to display topics, pass model, feat_names\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f06320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_top_words = 10\n",
    "# display_topics(model, feature_names, no_top_words)\n",
    "\n",
    "display_topics(nmf_2, vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define topic_terms_H \n",
    "# pass doc_topic_W\n",
    "\n",
    "H = pd.DataFrame(doc_topic.round(5),\n",
    "             index = ex_label,\n",
    "             columns = [\"component_1\",\"component_2\" ])\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f6341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4d583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (unsupervised)",
   "language": "python",
   "name": "unsupervised"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
